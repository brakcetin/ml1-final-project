
Artificial Intelligence (AI) is rapidly evolving and increasingly integrated into technologies that support people in their everyday lives. Among these advances, \textbf{Human Activity Recognition (HAR)} has emerged as a key area, enabling systems to understand and classify human movements automatically.

Modern HAR systems rely on two main sources of information: \textbf{computer vision} and \textbf{wearable sensors}. While vision-based approaches have achieved remarkable progress, wearable sensors like accelerometers and gyroscopes offer practical advantages. They are small, low-cost, energy-efficient, and fully portable, making them ideal for smartphones, or devices that people can carry everyday.

HAR is a \textbf{classification task}: the goal is to determine which activity a person is performing. These activities may include simple daily motions such as walking, sitting, standing, or running, as well as more specialized actions maybe relevant to medical monitoring, military applications, sports analysis, industrial safety and more.

% -----------------------------------------
% FIGURE PLACEHOLDER — replace with your own
% -----------------------------------------
 \begin{figure}[h]
     \centering
     \includegraphics[width=0.8\linewidth]{figures/ActivityClasiffication.png}
     \caption{General process of Human Activity Recognition (HAR).}
     \label{fig:har_process}
 \end{figure}
 %-----------------------------------------

To achieve accurate activity classification, HAR systems commonly employ \textbf{Machine Learning (ML)} techniques. These methods learn patterns directly from  data and can distinguish subtle differences between activities. As HAR datasets grow in size and complexity, ML-based approaches have become essential tools for building reliable, user centered recognition systems. Nowadays approachs are \textbf{Deep Learning} but our main target hear is to made clasification in traditional ML approachs.

\subsection{Dataset Description}

In this study we consider two widely used public datasets for human activity recognition: the \textbf{UCI HAR Dataset} and the \textbf{WISDM Dataset}. Both contain motion signals from wearable devices, but they differ in sampling frequency, sensor configuration, and acquisition protocol.

% ------------------------------------------------
% PLACEHOLDER FOR FIGURE (datasets illustration)
% ------------------------------------------------
% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.85\linewidth]{datasets_overview.png}
%     \caption{Examples of accelerometer and gyroscope raw signals from the HAR and WISDM datasets.}
%     \label{fig:datasets}
% \end{figure}
% ------------------------------------------------

\subsubsection{UCI HAR Dataset}

The UCI Human Activity Recognition dataset\footnote{\url{https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones}} provides synchronized \textbf{accelerometer} and \textbf{gyroscope} signals recorded from a smartphone fixed at the waist. All measurements are collected in \textbf{raw form}, capturing tri-axial linear acceleration and angular velocity (accelometor and gyroscopy respectively).

A important point of this dataset is that both sensors are recorded at the \textbf{same sampling frequency of 50\,Hz}. This uniformity simplifies preprocessing, since the accelerometer and gyroscope streams are inherently aligned and do not require temporal correction. The dataset also defines a \textbf{window length of 2.56\,s} (equivalent to 128 samples).

\subsubsection{WISDM Dataset}

The WISDM dataset\footnote{\url{https://www.cis.fordham.edu/wisdm/dataset.php}} contains accelerometer and gyroscope data collected from smartphones and smartwatches( omited in our study ) during free-form daily activities. Unlike the HAR dataset, WISDM’s sampling frequency is \textbf{20\,Hz}, so it's diferrent than UCI HAR frecuency.

Because accelerometer and gyroscope signals are acquired in raw form, they must ideally operate at the same frequency to guarantee correct temporal pairing. A common strategy to mitigate misalignment is to \textbf{interpolate} one signal so that both sensors share a unified sampling rate. In principle, interpolation can increase comparability between datasets and allow processing them under the same configuration (e.g., resampling WISDM to 50\,Hz).

However, interpolation introduces several considerations. First, it is \textbf{computationally expensive} when applied to long recordings or large datasets. Second, in window-based HAR pipelines—where statistical and frequency-domain features are extracted from fixed-length segments—the benefits of upsampling may be limited. Many window-level statistics (mean, variance, energy, entropy, etc.) do not significantly improve when artificially increasing the number of samples per window. For this reason, while interpolating WISDM from 20\,Hz to 50\,Hz is feasible and has been implemented, it may represent an \textbf{unnecessary computational cost} depending on the application.

\subsubsection{Sensor Requirements and Practical Notes}

Both datasets rely on two core inertial sensors:

\begin{itemize}
    \item \textbf{Accelerometer}: measures linear acceleration along the three axes.
    \item \textbf{Gyroscope}: measures angular velocity along the same three axes.
\end{itemize}

For creating a dataset, they must share a \textbf{common sampling frequency}. When this condition is not met (as in WISDM), interpolation becomes a possible solution, although not always required.

The UCI HAR dataset already satisfies this requirement, while WISDM requires additional processing depending on the target analysis pipeline.

\subsection{WISDM Signal Extraction and Preprocessing}

\subsubsection{Architecture: Adapter and Chain of Responsibility}

To ensure a flexible and reusable preprocessing system to anhother raws fluents, we adopt a framework structure based on an \textbf{Adapter} and a \textbf{Chain of Responsibility} pattern.

\begin{itemize}
    \item The \textbf{Adapter} reads the raw WISDM files and converts them into a unified internal representation.
    \item The \textbf{Chain of Responsibility} applies a sequence of processing steps (interpolation, filtering, masking, etc.), each step encapsulated as a separate component.
\end{itemize}


\begin{figure}[h]
\centering

\begin{tikzpicture}[
    font=\scriptsize,
    class/.style={rectangle, draw, rounded corners, minimum width=2.0cm, minimum height=0.55cm, align=center},
    abstract/.style={rectangle, draw, dashed, rounded corners, minimum width=2.0cm, minimum height=0.55cm, align=center},
    step/.style={rectangle, draw, minimum width=1.85cm, minimum height=0.50cm, align=center},
    inherit/.style={-{Triangle[length=3mm,open]}, thin},
    arrow/.style={-{Stealth[length=2mm]}, thin},
    node distance=0.60cm
]

% ============================
% ADAPTER LAYER
% ============================
\node[abstract] (adapter) {Adapter};
\node[class, below=0.5cm of adapter] (adapterw) {AdapterWISDM};
\node[class, below=0.5cm of adapterw] (ext) {ExternalDataset};

% AdapterWISDM <|-- Adapter (HERENCIA)
\draw[inherit] (adapterw) -- (adapter);

% AdapterWISDM --> ExternalDataset (NORMAL)
\draw[arrow] (adapterw) -- (ext);




% ============================
% PROCESSING INTERFACE
% ============================
\node[abstract, right=1.5cm of adapter] (proc) {ProcessingStep};

% NORMAL (NO herencia)
\draw[arrow] (proc.west) -- ++(-0.6,0) |- (adapter);


% ============================
% FILTERS - ALL INHERITANCE
% ============================
\node[abstract, below=0.55cm of proc] (filters) {Filters};

\node[step, below left=0.32cm and -0.15cm of filters] (interp) {Interpolation};
\node[step, below=0.27cm of filters] (smooth) {Smoothing};
\node[step, below right=0.32cm and -0.15cm of filters] (butter) {Butterworth};

% ProcessingStep <|-- Filters
\draw[inherit] (filters) -- (proc);

% Filters <|-- Steps
\draw[inherit] (interp) -- (filters);
\draw[inherit] (smooth) -- (filters);
\draw[inherit] (butter) -- (filters);

\draw[arrow] (proc.south east) .. controls +(0.8cm,-0.3cm) and +(0.8cm,0.3cm) .. (proc.north east);

% ============================
% ENRICHMENT - ALL INHERITANCE
% ============================
\node[abstract, right=1.7cm of proc] (enrich) {Enrichment};

\node[step, below left=0.30cm and -0.10cm of enrich] (grav) {Gravity};
\node[step, below=0.27cm of enrich] (tilt) {Tilt};
\node[step, below right=0.30cm and -0.10cm of enrich] (ratio) {Ratios};

% ProcessingStep <|-- Enrichment
\draw[inherit] (enrich) -- (proc);

% Enrichment <|-- Steps
\draw[inherit] (grav) -- (enrich);
\draw[inherit] (tilt) -- (enrich);
\draw[inherit] (ratio) -- (enrich);


% ============================
% MASKING - ALL INHERITANCE
% ============================
\node[abstract, below=0.80cm of enrich] (masking) {Masking};
\node[step, below=0.27cm of masking] (mask) {ProximityMask};

% ProcessingStep <|-- Masking
\draw[inherit] (masking) -- (proc);

% Masking <|-- Step
\draw[inherit] (mask) -- (masking);


% ============================
% WINDOWING - INHERITANCE
% ============================
\node[step, below=1.10cm of filters, xshift=3.6cm] (windowing) {Windowing};

% ProcessingStep <|-- Windowing
\draw[inherit] (windowing) -- (proc);

\end{tikzpicture}

\caption{Compact UML architecture using inheritance relationships for all processing components.}
\label{fig:uml_pipeline_compact}
\end{figure}


\subsubsection{WISDM Signal Extraction}

This is going to show how make a good dataset following this pipeline, this part will be used in a implementation

The WISDM dataset provides raw accelerometer and gyroscope measurements recorded at 20\,Hz for a smarthpone and smartwatches, for compatibility with HAR this last was ommited. However, the sensor streams are not always defined across the entire temporal dimension \(T\); gaps, missing bursts, or slight accelerometer–gyroscope misalignments are common.

To handle these issues, our pipeline first aligns both signals on a shared temporal  using \textbf{interpolation}. This step allows accelerometer and gyroscope data to be treated as synchronized multivariate time series, enabling consistent feature extraction.

After interpolation, two filters are applied sequentially:

\begin{enumerate}
    \item A \textbf{smoothing filter} (median or moving average), reducing impulsive noise and small fluctuations.
    \item A \textbf{Butterworth low-pass filter}, eliminating high-frequency components unlikely to contribute to human activity recognition.
\end{enumerate}

These steps produce smoother, more stable motion curves while preserving essential activity-related patterns so will be more realistic predicted values.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/wisdm_interpolation_filters.png}
    \caption{Example of filters. We can see the filter one,the smooting and butterworth functions}
    \label{fig:wisdm_signals}
\end{figure}


\subsubsection{Gravity–Body Separation and Derived Motion Features}

After interpolation and initial filtering, the accelerometer signal is decomposed into two physical components: a \textbf{gravity component} and a \textbf{body-motion component}. This separation is fundamental for correctly interpreting human posture and movement.

The decomposition is performed using a frequency-based approach:  
\begin{itemize}
    \item Frequencies \textbf{below 0.3\,Hz} correspond to the \textbf{gravity vector}, which reflects the orientation of the device in space.
    \item Frequencies \textbf{above 0.3\,Hz} represent the \textbf{body acceleration}, generated by voluntary movement.
\end{itemize}

This separation is essential because the gyroscope alone only measures \textbf{angular velocity} (rotation), but not device orientation. The gravity component, extracted from the accelerometer, enables the system to infer whether the subject is standing, sitting, lying down, or tilting in a particular direction—activities where orientation is the key discriminative factor.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/gravity_body_comparison.png}
    \caption{Comparison between gravity and body acceleration components for the x-axis. Gravity captures orientation, while body acceleration reflects dynamic motion.}
    \label{fig:gravity_body}
\end{figure}

\subsubsection{Additional Derived Measures: Jerk, Magnitudes, and Orientation}

In addition to gravity and body acceleration, several derived features are computed to enrich the representation of the dataset:

\begin{itemize}
    \item \textbf{Jerk}: the time derivative of acceleration or angular velocity.  
    It captures abrupt changes in motion, useful for detecting transitions and impacts.
    \[
        \text{Jerk}(t) = \frac{d\,\text{acc}(t)}{dt}
    \]
    Higher jerk values often correspond to dynamic activities such as running, jumping, or rapid turns.

    \item \textbf{Magnitude signals}:  
    \[
        \text{Mag}(t) = \sqrt{x^2 + y^2 + z^2}
    \]
    Magnitudes summarize 3D movement intensity and are invariant to device rotation.

    \item \textbf{Tilt/orientation angles}: Will be usefull in window part to the learnings process.
\end{itemize}
\subsubsection{Additional Derived Measures and High-level Motion Features}

Beyond gravity–body separation, several higher-level motion features are computed to enhance the representation of posture, orientation, and dynamic transitions. \textbf{These features are not part of the original WISDM or HAR datasets}; instead, \textbf{they are additional components introduced in our preprocessing pipeline} to enrich the sensor representation and improve classification performance. To get a more acurrate dataset is necesarry a research process in sensor analysis.

\paragraph{1. TiltFeatureExtractor (orientation from gravity).}
Device orientation is estimated using the gravity vector. For each timestamp, the pitch and roll angles are obtained as:
\[
\text{pitch} = \arctan2(-g_x,\; g_y^2 + g_z^2), \qquad
\text{roll}  = \arctan2(g_y,\; g_z).
\]
These angles describe how the device is tilted along two axes and are useful for distinguishing posture-related activities such as standing, sitting, or lying, where gravity orientation is the key discriminant.

\paragraph{2. GravityAlignedComponentsExtractor (vertical vs.\ horizontal motion).}
Body acceleration is projected onto the gravity direction to separate vertical and horizontal components.  
First, the gravity vector is normalized:
\[
\hat{g} = \frac{(g_x, g_y, g_z)}{\lVert g \rVert}.
\]
The vertical component of body acceleration is:
\[
a_{\text{vert}} = b_x \hat{g}_x + b_y \hat{g}_y + b_z \hat{g}_z,
\]
while the horizontal component is:
\[
a_{\text{horiz}} = \sqrt{\lVert b \rVert^2 - a_{\text{vert}}^2}.
\]
This decomposition provides information on movement aligned with gravity (e.g., sit-to-stand transitions) versus movement in the horizontal plane (e.g., walking).

\paragraph{3. RatioDynamicsExtractor (relative motion dynamics).}
Two useful ratios capture the relation between jerk (derivative of acceleration or angular velocity) and the underlying motion intensity:
\[
\begin{aligned}
\text{ratio}_{\text{jerk/body}} &=
\frac{\text{tBodyAccJerkMag}}{\text{tBodyAccMag} + \varepsilon}, \\
\text{ratio}_{\text{gyroJerk/gyro}} &=
\frac{\text{tBodyGyroJerkMag}}{\text{tBodyGyroMag} + \varepsilon}.
\end{aligned}
\]

These ratios highlight quick motion bursts relative to steady motion and are helpful for discriminating fast transitions from sustained activities.
.


\subsubsection{Real-support Masking and Proximity Constraints}

Interpolation estimates values between real measurements, but it does not create new information. If applied blindly, long regions without sensor readings would appear artificially continuous.

To prevent this, we apply a \textbf{proximity mask} (real-support mask). For every interpolated timestamp, we check whether an actual accelerometer and gyroscope measurement exists within a small time window \(\Delta t\). If no real sample is close enough, the interpolated point is \textbf{ignored} by all subsequent modules (windowing, feature extraction, classification).

\subsubsection{Windowing and Feature Extraction}

After preprocessing the sensor streams and enriching them with derived features, the next stage consists of segmenting the time series into fixed-length windows and computing a set of statistical and frequency-domain descriptors. This step transforms raw temporal data into structured samples suitable for traditional machine learning models.

\subsubssubection{Sliding Window Generation}

Following the conventions established in the UCI HAR benchmark, each signal is segmented using windows of \textbf{2.56\,s} duration. At a reference sampling frequency of \textbf{50\,Hz}, this corresponds to:
\[
128\ \text{samples per window}.
\]

Windows are generated with a \textbf{50\% overlap}:
\[
\text{stride} = 1.28\,\text{s} \quad (64\ \text{samples}),
\]
which increases temporal continuity and ensures that relevant motion transitions are not missed, and could be usefull in DL approachs.


\subsububsection{Statistical Time-Domain Features}

For each accepted window, classical time-domain statistics are computed over all signals (accelerometer, gyroscope, body components, gravity components, jerk, magnitudes, etc.). These include:
\begin{itemize}
    \item mean, median, variance, standard deviation,
    \item minimum, maximum, range,
    \item interquartile range,
    \item signal magnitude area (SMA),
    \item energy,
    \item zero-crossing rate (for oscillatory signals),
    \item autoregressive coefficients (optional).
\end{itemize}

These descriptors summarize the global behaviour and variability of motion within each window.

\subsubsubsection{Frequency-Domain Features via FFT}

In addition to time-domain statistics, frequency-domain information is extracted using a \textbf{Fast Fourier Transform (FFT)} applied independently to each axis or derived feature. 

The magnitude spectrum is obtained and normalized. From it, we compute:
\begin{itemize}
    \item dominant frequency components,
    \item energy in predefined frequency bands,
    \item spectral entropy,
    \item centroid and bandwidth,
    \item frequency-domain means and medians.
\end{itemize}

This spectral analysis provides insight into periodicity and vibration patterns, which are crucial for distinguishing activities such as walking, running, and transitions between static and dynamic states. 


This representation is particularly advantageous for classical machine learning models, which lack temporal memory and therefore require structured descriptors to compensate for the absence of contextual awareness.

Recurrent neural networks such as LSTM or BiLSTM models, however, possess internal memory mechanisms that allow them to retain information from previous observations. For these architectures, providing the raw or minimally processed sensor streams is not only sufficient but often preferable, since temporal continuity enables the model to infer whether the subject had been sitting, standing, turning, or transitioning long before the current window begins. In this sense, strict segmentation into independent windows inevitably discards part of the temporal context that deep recurrent models are capable of exploiting.

Nevertheless, some engineered features—particularly frequency-domain components and spectral bands—remain valuable even for deep models, as they highlight periodic and harmonic structures that may not be trivially learned from raw data alone. Thus, while feature engineering is essential for traditional ML pipelines, its role in deep learning presents a nuanced trade-off between preserving raw temporal information and emphasizing discriminative patterns.


