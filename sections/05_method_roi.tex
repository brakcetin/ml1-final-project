The goal of this approach is to design a model capable of stronger subject-level generalisation. 
For this purpose, the \textit{Subject} identifier included in the dataset is explicitly used to separate 
instances belonging to different individuals. This separation plays a central role in all stages of 
the pipeline: dataset splitting, cross-validation design, and evaluation on unseen subjects.

% =========================================================
\subsection{Dataset Description}
% =========================================================

We use the UCI HAR dataset, which contains time-windowed inertial signals recorded from a 
smartphone worn on the waist. Each instance is represented by 561 aggregated statistical 
features and labelled with one of six activities: \textit{WALKING}, \textit{WALKING\_UPSTAIRS}, 
\textit{WALKING\_DOWNSTAIRS}, \textit{SITTING}, \textit{STANDING}, \textit{LAYING}.

Before splitting the dataset, we verify that samples are not unevenly distributed across 
subjects. Figure~\ref{fig:samples_per_subject} shows the number of samples per person in the 
original dataset.

% FIGURE PLACEHOLDER
\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/samokes.png}
    \caption{Number of samples per subject in the UCI HAR dataset.}
    \label{fig:samples_per_subject}
\end{figure}

The original train/test split included instances from nearly all subjects in both sets, meaning 
the model would be tested on data from people also present in the training set. To enforce true 
subject-disjoint evaluation, we merge the original splits and re-split the dataset as follows:

\begin{itemize}
    \item \textbf{Test1} ($\approx$30\%): around 10 subjects, \emph{all} their instances appear only here.
    \item \textbf{Train} ($\approx$65\%): the remaining $\approx$20 subjects (completely disjoint from Test1).
    \item \textbf{Test2} ($\approx$5\%): a small subsample of Train, used to evaluate generalisation to new 
    windows from already-seen individuals.
\end{itemize}

After splitting, the dataset distribution is approximately: Train $\approx$ 6300 samples, 
Test1 $\approx$ 3400, Test2 $\approx$ 500.

Figure~\ref{fig:class_distribution_splits} shows class frequencies across the three splits.

% FIGURE PLACEHOLDER
\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/distributionsplit.png}
    \caption{Class distribution in Train, Test1 (subject-disjoint), and Test2 (mixed).}
    \label{fig:class_distribution_splits}
\end{figure}



% =========================================================
\subsection{Data Preprocessing}
% =========================================================

All models share the same preprocessing pipeline. We apply \textbf{Z-score normalisation per 
feature} across the entire dataset to ensure comparability between algorithms.  

We do not apply min–max scaling, outlier filtering, or temporal operations, since the HAR 
features are already engineered and Z-score stabilises optimisation for gradient-based models 
(ANNs, SVMs). Although trees and kNN do not strictly require normalisation, using a unified 
pipeline ensures a fair comparison.



% =========================================================
\subsection{Experimental Setup}
% =========================================================

\subsubsection{Supervised Feature Selection}

We perform feature selection on the training set using one-way ANOVA F-scores.  
For each feature, the score measures the ratio between inter-class and intra-class variance.  
Sorting features by decreasing F-score produces a ranking such as the one displayed in 
Fig.~\ref{fig:fscore_ranking}.

% FIGURE PLACEHOLDER
\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/Anova.png}
    \caption{Ranking of features by ANOVA F-score.}
    \label{fig:fscore_ranking}
\end{figure}

We evaluate several values of $N$ (top-$N$ features) using a fixed ANN architecture with 
subject-disjoint cross-validation. Results show that using all 561 features yields the highest 
CV accuracy, but subsets of 200--300 features significantly reduce training times with minimal 
accuracy loss. The selected feature mask is then applied consistently to Train, Test1, and Test2.



\subsubsection{Subject-Disjoint Cross-Validation}

To maintain subject independence during model selection, folds are generated at the subject 
level. A custom method shuffles the subjects in the training split and assigns them in 
round-robin fashion to one of the $k$ folds.

Although the training split contains approximately 20 subjects, performing 20-fold CV was too 
expensive computationally, so we adopt a 10-fold protocol (folds of two subjects).  

This ensures each fold evaluates on completely unseen individuals, mirroring the Test1 
scenario.



% =========================================================
\subsection{Model Experimentation}
% =========================================================

All four ML methods studied in the course are evaluated under the same subject-wise CV 
procedure and feature-selection mask.

\subsubsection{Artificial Neural Networks (ANN)}

We test eight fully connected architectures with one or two hidden layers:  
\[
[64],\; [128],\; [256],\; [64,32],\; [128,64],\; [256,128],\; [128,128],\; [256,256].
\]

Training settings: Adam optimiser, softmax cross-entropy loss, learning rate 0.005, 
maxEpochs=350, early stopping with patience=10, validation ratio 0.2.  
For each architecture we compute mean$\pm$std accuracy and F1-score across the 10 folds and 
select the best-performing topology.

\subsubsection{Support Vector Machines (SVM)}

We evaluate eight configurations:

\begin{itemize}
    \item \textbf{Linear kernel:} $C \in \{0.1, 1, 10\}$ (3 configs)
    \item \textbf{RBF kernel:} $(C,\gamma) \in \{(1,0.01), (10,0.01), (10,0.001)\}$ (3 configs)
    \item \textbf{Polynomial kernel:} $C=1$, degree $\in \{2,3\}$ (2 configs)
\end{itemize}

Each model is trained on subject-disjoint folds and evaluated via CV accuracy.

\subsubsection{Decision Trees (DT)}

We vary the maximum depth $\in \{3,5,7,9,11,13\}$.  
Shallow trees underfit, while overly deep trees overfit; intermediate depths (7--9) provide the 
best trade-off.

\subsubsection{k-Nearest Neighbours (kNN)}

We test $k \in \{1,3,5,7,9,11\}$.  
Z-score normalisation ensures Euclidean distance behaves consistently. Moderate values such 
as $k=9$ or $k=11$ yield the best accuracy.



% =========================================================
\subsection{Model Evaluation and Ensemble Method}
% =========================================================

After selecting the best configuration for ANN, SVM, DT, and kNN, we retrain each model on 
the full training set (after feature selection) and evaluate them on Test1 (subject-disjoint) and 
Test2 (mixed).

Figures~\ref{fig:confusion_test1} and~\ref{fig:confusion_test2} illustrate confusion matrices for the 
best-performing models on each evaluation split.

%% --- TABLE: TEST1 MODEL SUMMARY ---
\begin{table}[h]
  \centering
  \caption{Model performance summary on Test1 (subject-disjoint).}
  \label{tab:test1_model_summary}
  \resizebox{0.85\columnwidth}{!}{%
  \begin{tabular}{lcc}
    \toprule
    \textbf{Model} & \textbf{Accuracy} & \textbf{Weighted F1} \\
    \midrule
    ANN & 0.9444 & 0.9444 \\
    SVM & 0.9459 & 0.9458 \\
    kNN & 0.8844 & 0.8843 \\
    DT  & 0.8780 & 0.8775 \\
    \textbf{Ensemble} & \textbf{0.9470} & \textbf{0.9469} \\
    \bottomrule
  \end{tabular}
  }
\end{table}


% --- TABLE: TEST2 MODEL SUMMARY ---
\begin{table}[h]
  \centering
  \caption{Model performance summary on Test2 (mixed subjects).}
  \label{tab:test2_model_summary}
  \resizebox{0.85\columnwidth}{!}{%
  \begin{tabular}{lcc}
    \toprule
    \textbf{Model} & \textbf{Accuracy} & \textbf{Weighted F1} \\
    \midrule
    ANN & 0.9767 & 0.9767 \\
    SVM & 0.9825 & 0.9825 \\
    kNN & 0.9592 & 0.9591 \\
    DT  & 0.9437 & 0.9434 \\
    \textbf{Ensemble} & \textbf{0.9825} & \textbf{0.9825} \\
    \bottomrule
  \end{tabular}
  }
\end{table}


Finally, we construct a simple \textbf{hard-voting ensemble} combining the best ANN, best SVM, 
and best kNN. For each sample, the predicted class is chosen by unweighted majority vote.  
Decision trees are excluded from the ensemble due to lower performance in this setting.

In addition to these observations, it is worth highlighting that the subject-disjoint evaluation 
provides a more faithful approximation of real deployment scenarios, where models must 
operate on new users whose motion patterns, phone placement habits, and biomechanical 
signatures differ from those seen during training. The clear performance gap between Test1 
and Test2 quantifies this challenge directly: even strong models such as SVMs and ANNs 
exhibit a non-negligible degradation when confronted with unseen subjects. This reinforces the 
need for HAR systems that are robust to user variability, either through improved 
representation learning, personalised calibration strategies, or domain-adaptation techniques.

Moreover, the experimental results show that no single model fully dominates all others across 
evaluation conditions. The fact that a simple majority-vote ensemble systematically reduces a 
portion of the remaining classification errors indicates that different model families capture 
complementary aspects of the signal space. This suggests that hybrid or hierarchical HAR 
architectures may benefit from combining discriminative boundaries learned by SVMs with the 
latent representations extracted by ANNs, while exploiting the local decision stability inherent 
to kNN. Overall, these findings underline that subject-general HAR remains an open problem, 
and that carefully designed pipelines—integrating feature selection, subject-wise evaluation, 
and model ensembling—are essential for developing reliable and deployable systems.
